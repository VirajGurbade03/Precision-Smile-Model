{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\VIRAJ GURBADE\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 70\u001b[0m\n\u001b[0;32m     58\u001b[0m train_datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(\n\u001b[0;32m     59\u001b[0m     rotation_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m,       \u001b[38;5;66;03m# Random rotation\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     width_shift_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,   \u001b[38;5;66;03m# Horizontal shift\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m     fill_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m'\u001b[39m      \u001b[38;5;66;03m# Fill empty pixels\u001b[39;00m\n\u001b[0;32m     66\u001b[0m )\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Use data augmentation to expand the dataset\u001b[39;00m\n\u001b[0;32m     69\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m train_datagen\u001b[38;5;241m.\u001b[39mflow(\n\u001b[1;32m---> 70\u001b[0m     \u001b[43mimages\u001b[49m, \n\u001b[0;32m     71\u001b[0m     masks, \n\u001b[0;32m     72\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Batch size of 1 since we are working with one image\u001b[39;00m\n\u001b[0;32m     73\u001b[0m )\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Build and compile the model\u001b[39;00m\n\u001b[0;32m     76\u001b[0m model \u001b[38;5;241m=\u001b[39m edge_tracing_unet()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'images' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure a clean TensorFlow session\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Step 2: Simplified U-Net for Edge Tracing\n",
    "def edge_tracing_unet(input_size=(256, 256, 1)):\n",
    "    \"\"\"\n",
    "    Build a U-Net model for edge tracing.\n",
    "\n",
    "    Args:\n",
    "        input_size (tuple): Input image size.\n",
    "\n",
    "    Returns:\n",
    "        Model: Compiled U-Net model.\n",
    "    \"\"\"\n",
    "    inputs = Input(input_size)\n",
    "\n",
    "    # Encoder\n",
    "    c1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    c1 = Conv2D(32, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = Conv2D(64, (3, 3), activation='relu', padding='same')(p1)\n",
    "    c2 = Conv2D(64, (3, 3), activation='relu', padding='same')(c2)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    # Bottleneck\n",
    "    c3 = Conv2D(128, (3, 3), activation='relu', padding='same')(p2)\n",
    "    c3 = Conv2D(128, (3, 3), activation='relu', padding='same')(c3)\n",
    "\n",
    "    # Decoder\n",
    "    u4 = UpSampling2D((2, 2))(c3)\n",
    "    u4 = Concatenate()([u4, c2])\n",
    "    c4 = Conv2D(64, (3, 3), activation='relu', padding='same')(u4)\n",
    "    c4 = Conv2D(64, (3, 3), activation='relu', padding='same')(c4)\n",
    "\n",
    "    u5 = UpSampling2D((2, 2))(c4)\n",
    "    u5 = Concatenate()([u5, c1])\n",
    "    c5 = Conv2D(32, (3, 3), activation='relu', padding='same')(u5)\n",
    "    c5 = Conv2D(32, (3, 3), activation='relu', padding='same')(c5)\n",
    "\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c5)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Example: Use the images and masks directly without the need to load from directories\n",
    "# Assuming `images` and `masks` are numpy arrays with shapes (num_samples, 256, 256, 1)\n",
    "\n",
    "# Step 3: Data Augmentation and Model Training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=15,       # Random rotation\n",
    "    width_shift_range=0.1,   # Horizontal shift\n",
    "    height_shift_range=0.1,  # Vertical shift\n",
    "    zoom_range=0.2,          # Random zoom\n",
    "    shear_range=0.1,         # Shear transformation\n",
    "    horizontal_flip=True,    # Horizontal flip\n",
    "    fill_mode='nearest'      # Fill empty pixels\n",
    ")\n",
    "\n",
    "# Use data augmentation to expand the dataset\n",
    "train_generator = train_datagen.flow(\n",
    "    images, \n",
    "    masks, \n",
    "    batch_size=1  # Batch size of 1 since we are working with one image\n",
    ")\n",
    "\n",
    "# Build and compile the model\n",
    "model = edge_tracing_unet()\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 3: Model Training\n",
    "history = model.fit(\n",
    "    train_generator, \n",
    "    epochs=50,  # Train for more epochs as we are using augmented data\n",
    "    steps_per_epoch=100,  # Generate 100 augmented samples per epoch\n",
    "    validation_data=(images, masks)  # Validate on the same images\n",
    ")\n",
    "\n",
    "# Step 4: Visualize Training Results\n",
    "loss, accuracy = model.evaluate(images, masks)\n",
    "print(f\"Training Loss: {loss:.4f}, Training Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Visualize the predicted mask for the same input\n",
    "predicted_mask = model.predict(images)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(images[0].squeeze(), cmap='gray')\n",
    "plt.title(\"Input Image\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(predicted_mask[0].squeeze(), cmap='gray')\n",
    "plt.title(\"Predicted Edge Mask\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'D:\\\\COLLEGE\\\\Projects\\\\AI-Driven Cephalometric Analysis\\\\Percision Smile\\\\Tracesing_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 74\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Load the images and masks\u001b[39;00m\n\u001b[0;32m     73\u001b[0m input_image \u001b[38;5;241m=\u001b[39m load_and_process_image(input_image_path)\n\u001b[1;32m---> 74\u001b[0m mask_image \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_process_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_image_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Convert images to 4D array (batch size, height, width, channels)\u001b[39;00m\n\u001b[0;32m     77\u001b[0m X_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(input_image, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add batch dimension\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 68\u001b[0m, in \u001b[0;36mload_and_process_image\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_and_process_image\u001b[39m(image_path):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    Load and preprocess image by resizing to 256x256 and converting to grayscale.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m        numpy array: Processed image.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgrayscale\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m     img_array \u001b[38;5;241m=\u001b[39m img_to_array(img) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img_array\n",
      "File \u001b[1;32mc:\\Users\\VIRAJ GURBADE\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\image_utils.py:235\u001b[0m, in \u001b[0;36mload_img\u001b[1;34m(path, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, pathlib\u001b[38;5;241m.\u001b[39mPath):\n\u001b[0;32m    234\u001b[0m         path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(path\u001b[38;5;241m.\u001b[39mresolve())\n\u001b[1;32m--> 235\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    236\u001b[0m         img \u001b[38;5;241m=\u001b[39m pil_image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(f\u001b[38;5;241m.\u001b[39mread()))\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'D:\\\\COLLEGE\\\\Projects\\\\AI-Driven Cephalometric Analysis\\\\Percision Smile\\\\Tracesing_datasets'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure a clean TensorFlow session\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Step 2: Simplified U-Net for Edge Tracing\n",
    "def edge_tracing_unet(input_size=(256, 256, 1)):\n",
    "    \"\"\"\n",
    "    Build a U-Net model for edge tracing.\n",
    "\n",
    "    Args:\n",
    "        input_size (tuple): Input image size.\n",
    "\n",
    "    Returns:\n",
    "        Model: Compiled U-Net model.\n",
    "    \"\"\"\n",
    "    inputs = Input(input_size)\n",
    "\n",
    "    # Encoder\n",
    "    c1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    c1 = Conv2D(32, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = Conv2D(64, (3, 3), activation='relu', padding='same')(p1)\n",
    "    c2 = Conv2D(64, (3, 3), activation='relu', padding='same')(c2)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    # Bottleneck\n",
    "    c3 = Conv2D(128, (3, 3), activation='relu', padding='same')(p2)\n",
    "    c3 = Conv2D(128, (3, 3), activation='relu', padding='same')(c3)\n",
    "\n",
    "    # Decoder\n",
    "    u4 = UpSampling2D((2, 2))(c3)\n",
    "    u4 = Concatenate()([u4, c2])\n",
    "    c4 = Conv2D(64, (3, 3), activation='relu', padding='same')(u4)\n",
    "    c4 = Conv2D(64, (3, 3), activation='relu', padding='same')(c4)\n",
    "\n",
    "    u5 = UpSampling2D((2, 2))(c4)\n",
    "    u5 = Concatenate()([u5, c1])\n",
    "    c5 = Conv2D(32, (3, 3), activation='relu', padding='same')(u5)\n",
    "    c5 = Conv2D(32, (3, 3), activation='relu', padding='same')(c5)\n",
    "\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c5)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Load the input image and corresponding mask (training image and mask)\n",
    "input_image_path = r\"D:\\COLLEGE\\Projects\\AI-Driven Cephalometric Analysis\\Percision Smile\\Data_sets\\p2 gaurav2.jpeg\"\n",
    "mask_image_path = r\"D:\\COLLEGE\\Projects\\AI-Driven Cephalometric Analysis\\Percision Smile\\Tracesing_datasets\"\n",
    "\n",
    "def load_and_process_image(image_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess image by resizing to 256x256 and converting to grayscale.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "    \n",
    "    Returns:\n",
    "        numpy array: Processed image.\n",
    "    \"\"\"\n",
    "    img = load_img(image_path, target_size=(256, 256), color_mode='grayscale')\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    return img_array\n",
    "\n",
    "# Load the images and masks\n",
    "input_image = load_and_process_image(input_image_path)\n",
    "mask_image = load_and_process_image(mask_image_path)\n",
    "\n",
    "# Convert images to 4D array (batch size, height, width, channels)\n",
    "X_train = np.expand_dims(input_image, axis=0)  # Add batch dimension\n",
    "y_train = np.expand_dims(mask_image, axis=0)   # Add batch dimension\n",
    "\n",
    "# Data Augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=15,       # Random rotation\n",
    "    width_shift_range=0.1,   # Horizontal shift\n",
    "    height_shift_range=0.1,  # Vertical shift\n",
    "    zoom_range=0.2,          # Random zoom\n",
    "    shear_range=0.1,         # Shear transformation\n",
    "    horizontal_flip=True,    # Horizontal flip\n",
    "    fill_mode='nearest'      # Fill empty pixels\n",
    ")\n",
    "\n",
    "# Use data augmentation to expand the dataset\n",
    "train_generator = train_datagen.flow(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    batch_size=1  # Batch size of 1 since we are working with one image\n",
    ")\n",
    "\n",
    "# Build and compile the model\n",
    "model = edge_tracing_unet()\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 3: Model Training\n",
    "history = model.fit(\n",
    "    train_generator, \n",
    "    epochs=50,  # Train for more epochs as we are using augmented data\n",
    "    steps_per_epoch=100,  # Generate 100 augmented samples per epoch\n",
    "    validation_data=(X_train, y_train)  # Validate on the same images\n",
    ")\n",
    "\n",
    "# Step 4: Visualize Training Results\n",
    "loss, accuracy = model.evaluate(X_train, y_train)\n",
    "print(f\"Training Loss: {loss:.4f}, Training Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Visualize the predicted mask for the same input\n",
    "predicted_mask = model.predict(X_train)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(X_train[0].squeeze(), cmap='gray')\n",
    "plt.title(\"Input Image\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(predicted_mask[0].squeeze(), cmap='gray')\n",
    "plt.title(\"Predicted Edge Mask\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input data in `NumpyArrayIterator` should have rank 4. You passed an array with shape (144, 256, 256, 1, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 119\u001b[0m\n\u001b[0;32m    108\u001b[0m train_datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(\n\u001b[0;32m    109\u001b[0m     rotation_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m,       \u001b[38;5;66;03m# Random rotation\u001b[39;00m\n\u001b[0;32m    110\u001b[0m     width_shift_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,   \u001b[38;5;66;03m# Horizontal shift\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    115\u001b[0m     fill_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m'\u001b[39m      \u001b[38;5;66;03m# Fill empty pixels\u001b[39;00m\n\u001b[0;32m    116\u001b[0m )\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Use data augmentation to expand the dataset\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_datagen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Batch size of 1 since we are working with one image\u001b[39;49;00m\n\u001b[0;32m    123\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# Build and compile the model\u001b[39;00m\n\u001b[0;32m    126\u001b[0m model \u001b[38;5;241m=\u001b[39m edge_tracing_unet()\n",
      "File \u001b[1;32mc:\\Users\\VIRAJ GURBADE\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:1103\u001b[0m, in \u001b[0;36mImageDataGenerator.flow\u001b[1;34m(self, x, y, batch_size, shuffle, sample_weight, seed, save_to_dir, save_prefix, save_format, ignore_class_split, subset)\u001b[0m\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflow\u001b[39m(\n\u001b[0;32m   1090\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1091\u001b[0m     x,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1101\u001b[0m     subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1102\u001b[0m ):\n\u001b[1;32m-> 1103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNumpyArrayIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1106\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_to_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_to_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1113\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_class_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_class_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\VIRAJ GURBADE\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:612\u001b[0m, in \u001b[0;36mNumpyArrayIterator.__init__\u001b[1;34m(self, x, y, image_data_generator, batch_size, shuffle, sample_weight, seed, data_format, save_to_dir, save_prefix, save_format, subset, ignore_class_split, dtype)\u001b[0m\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_misc \u001b[38;5;241m=\u001b[39m x_misc\n\u001b[0;32m    611\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m--> 612\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    613\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput data in `NumpyArrayIterator` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    614\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshould have rank 4. You passed an array \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    615\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    616\u001b[0m     )\n\u001b[0;32m    617\u001b[0m channels_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchannels_last\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[channels_axis] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m}:\n",
      "\u001b[1;31mValueError\u001b[0m: Input data in `NumpyArrayIterator` should have rank 4. You passed an array with shape (144, 256, 256, 1, 1)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Ensure a clean TensorFlow session\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Step 2: Simplified U-Net for Edge Tracing\n",
    "def edge_tracing_unet(input_size=(256, 256, 1)):\n",
    "    \"\"\"\n",
    "    Build a U-Net model for edge tracing.\n",
    "\n",
    "    Args:\n",
    "        input_size (tuple): Input image size.\n",
    "\n",
    "    Returns:\n",
    "        Model: Compiled U-Net model.\n",
    "    \"\"\"\n",
    "    inputs = Input(input_size)\n",
    "\n",
    "    # Encoder\n",
    "    c1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    c1 = Conv2D(32, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = Conv2D(64, (3, 3), activation='relu', padding='same')(p1)\n",
    "    c2 = Conv2D(64, (3, 3), activation='relu', padding='same')(c2)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    # Bottleneck\n",
    "    c3 = Conv2D(128, (3, 3), activation='relu', padding='same')(p2)\n",
    "    c3 = Conv2D(128, (3, 3), activation='relu', padding='same')(c3)\n",
    "\n",
    "    # Decoder\n",
    "    u4 = UpSampling2D((2, 2))(c3)\n",
    "    u4 = Concatenate()([u4, c2])\n",
    "    c4 = Conv2D(64, (3, 3), activation='relu', padding='same')(u4)\n",
    "    c4 = Conv2D(64, (3, 3), activation='relu', padding='same')(c4)\n",
    "\n",
    "    u5 = UpSampling2D((2, 2))(c4)\n",
    "    u5 = Concatenate()([u5, c1])\n",
    "    c5 = Conv2D(32, (3, 3), activation='relu', padding='same')(u5)\n",
    "    c5 = Conv2D(32, (3, 3), activation='relu', padding='same')(c5)\n",
    "\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c5)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Load the input image and corresponding mask dataset\n",
    "input_image_path = r\"D:\\COLLEGE\\Projects\\AI-Driven Cephalometric Analysis\\Percision Smile\\Data_sets\\p2 gaurav2.jpeg\"\n",
    "mask_image_path = r\"D:\\COLLEGE\\Projects\\AI-Driven Cephalometric Analysis\\Percision Smile\\Tracesing_datasets\"\n",
    "\n",
    "def load_and_process_image(image_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess image by resizing to 256x256 and converting to grayscale.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "    \n",
    "    Returns:\n",
    "        numpy array: Processed image.\n",
    "    \"\"\"\n",
    "    img = load_img(image_path, target_size=(256, 256), color_mode='grayscale')\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    return img_array\n",
    "\n",
    "\n",
    "def load_and_process_images_from_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess all images from a directory by resizing to 256x256 \n",
    "    and converting to grayscale.\n",
    "    \n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing images.\n",
    "    \n",
    "    Returns:\n",
    "        numpy array: Array of processed images.\n",
    "    \"\"\"\n",
    "    image_list = []\n",
    "    \n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.jpeg') or filename.endswith('.png'):\n",
    "            image_path = os.path.join(directory_path, filename)\n",
    "            img = load_img(image_path, target_size=(256, 256), color_mode='grayscale')\n",
    "            img_array = img_to_array(img) / 255.0\n",
    "            image_list.append(img_array)\n",
    "    \n",
    "    return np.array(image_list)\n",
    "\n",
    "# Load the images and masks\n",
    "input_image = load_and_process_image(input_image_path)\n",
    "mask_images = load_and_process_images_from_directory(mask_image_path)\n",
    "\n",
    "# Ensure the input image has the same number of samples as the mask images\n",
    "X_train = np.repeat(np.expand_dims(input_image, axis=0), mask_images.shape[0], axis=0)  # Repeat input image\n",
    "\n",
    "# Convert images to 4D arrays (batch size, height, width, channels)\n",
    "X_train = X_train[..., np.newaxis]  # Add channel dimension\n",
    "y_train = mask_images[..., np.newaxis]  # Add channel dimension\n",
    "\n",
    "# Data Augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=15,       # Random rotation\n",
    "    width_shift_range=0.1,   # Horizontal shift\n",
    "    height_shift_range=0.1,  # Vertical shift\n",
    "    zoom_range=0.2,          # Random zoom\n",
    "    shear_range=0.1,         # Shear transformation\n",
    "    horizontal_flip=True,    # Horizontal flip\n",
    "    fill_mode='nearest'      # Fill empty pixels\n",
    ")\n",
    "\n",
    "# Use data augmentation to expand the dataset\n",
    "train_generator = train_datagen.flow(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    batch_size=1  # Batch size of 1 since we are working with one image\n",
    ")\n",
    "\n",
    "# Build and compile the model\n",
    "model = edge_tracing_unet()\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 3: Model Training\n",
    "history = model.fit(\n",
    "    train_generator, \n",
    "    epochs=50,  # Train for more epochs as we are using augmented data\n",
    "    steps_per_epoch=100,  # Generate 100 augmented samples per epoch\n",
    "    validation_data=(X_train, y_train)  # Validate on the same images\n",
    ")\n",
    "\n",
    "# Step 4: Visualize Training Results\n",
    "loss, accuracy = model.evaluate(X_train, y_train)\n",
    "print(f\"Training Loss: {loss:.4f}, Training Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Visualize the predicted mask for the same input\n",
    "predicted_mask = model.predict(X_train)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(X_train[0].squeeze(), cmap='gray')\n",
    "plt.title(\"Input Image\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(predicted_mask[0].squeeze(), cmap='gray')\n",
    "plt.title(\"Predicted Edge Mask\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
